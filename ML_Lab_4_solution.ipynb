{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FhWM7M4QuSw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "training_size = 100000\n",
        "validation_size = 50000\n",
        "test_size = 50000\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "  return 100*x**3 - 17*x**2 + 3*x + 11"
      ],
      "metadata": {
        "id": "6Z_6lM3GSpZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Generation:**\n",
        "We generate the data according to the following function:\n",
        "> $f(x) = 100x^3 - 17x^2 + 3x + 11$\n",
        "\n",
        "But in real life, the data don't abide by such a nice polynomial function. There are some noises. Let's say that the noise follows a normal distribution with mean = 0, and standard deviation = 0.1 \n",
        "So, we will generate our data using the following function:\n",
        "> $g(x) = 100x^3 - 17x^2 + 3x + 11 + ϵ $\n",
        "\n",
        "Where, $ϵ \\sim \\mathcal{N}(0, 0.1)$. Let's say, our training data size is $n$. Then we generate $x_{1}, x_{2}, \\cdots, x_{n}$ from a uniform distribution $[0, 1]$. Then for each $x_{i}$, we compute $y_{i} = g(x_{i})$. \n",
        "\n",
        "Now, let our hypothesis be a cubic polynomial of $x$. Hence, $h(x) = ax^3 + bx^2 + cx + d$. From the training dataset, we can create $n$ linear equations with 4 unknowns $(a, b, c, d)$. \n",
        " >> $ax_{1}^3 + bx_{1}^2 + cx_{1} + d = y_{1}$\n",
        "\n",
        " >> $ax_{2}^3 + bx_{2}^2 + cx_{2} + d = y_{2}$\n",
        "\n",
        " >> ...\n",
        "\n",
        " >> $ax_{n}^3 + bx_{n}^2 + cx_{n} + d = y_{n}$\n",
        "\n",
        " Notice that this is a system of linear equations. So, we can solve this problem of polynomial regression using just linear regression. And linear regression can be solved using many techniques such as least squares, gradient descent, calculus and so on. The data generation part is shown below."
      ],
      "metadata": {
        "id": "i6Nlw3ZYQ7Uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(a, b, mean, std, size):\n",
        "  x = np.random.uniform(a, b, size) # takes 'size' number of random values from the range [a, b]\n",
        "  y = [f(a) for a in x] # for each x, calculates f(x)\n",
        "  noise = np.random.normal(mean, std, size) # for each x, calculates some noise\n",
        "  y = y + noise # adds the noise to f(x)\n",
        "  y = np.array(y)\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "8n1ALiDt5GDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, train_y = generate(0, 1, 0, 0.1, training_size)\n",
        "validation_x, validation_y = generate(0, 1, 0, 0.1, validation_size)\n",
        "test_x, test_y = generate(0, 1, 0, 0.1, test_size)"
      ],
      "metadata": {
        "id": "pryqFsGlRD6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regularization and Weight Decay**\n",
        "In linear regression our objective function is the mean square error. We want to minimize that. \n",
        ">> $E = \\frac{\\sum_{i = 1}^{n} (y_{i} - h(x_{i}))^{2}}{n}$\n",
        "\n",
        "But, a complex hypothesis might minimize this empirical error. And we might pick this hypothesis over a simpler one. So, in order to penalize more complex hypothesis we can minimize the following objective function.\n",
        "\n",
        ">> $\\frac{\\sum_{i = 1}^{n} (y_{i} - h(x_{i}))^{2}}{n} + \\alpha$ $c(h)$\n",
        "\n",
        "Where $c(h)$ means the complexity of the hypothesis $h$. There are many ways of defining the complexity of a hypothesis. Two of the most popular ways is to take:\n",
        "\n",
        "$c(h) = |w_{1}|^{2} + |w_{2}|^{2} + \\cdots + |w_{n}|^{2}$, or\n",
        "\n",
        "\n",
        "$c(h) = |w_{1}| + |w_{2}| + \\cdots + |w_{n}|$\n",
        "\n",
        "where $w_{i}$ are the parameters of the hypothesis. If we use the former formula as the complexity, the regression problem is called ridge regression and if we use the second formula, the regression problem is called lasso. This sort fo regularization is called 'weight decay'.\n"
      ],
      "metadata": {
        "id": "bJjYEmeUanAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation**: Our model includes the choice of the degree of the polynomial, coefficients of the polynomial, $\\alpha$ the regularization parameter, $\\lambda$ the learning rate etc. How do we know which combinations of these parameters are going to yield the best result? We cannot rely on the empirical error to decide which model performs the best. Because our models were trained according to the training data. But what if we had access to some spare data? Let's call this the validation data. In that case, we could test our different models in this validation data and choose the best performing data? This process is called validation."
      ],
      "metadata": {
        "id": "_8_WOGPZOHl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross Validation**: Sometimes, we do not have access to spare data. So, we need to treat part of our training data as the validation data. Sometimes, the training data is split into $k$ parts. Then we treat each data segment as the validation data and the rest $k-1$ segments are treated as the training data. Then the final validation error is the average of the validation errors in all of these $k$ validation data segments. And finally all of the models go through this k-fold cross validation. The model that performs the best is chosen for the testing phase."
      ],
      "metadata": {
        "id": "MGIlJw1IQvpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Task**\n",
        "\n",
        "***Part 1:***\n",
        "1.   You have 6 models. \n",
        "> * linear polynomial regressor of degree 4\n",
        "> * linear polynomial regressor of degree 3\n",
        "> * lasso polynomial regressor of degree 4 with $\\alpha$ = 0.01\n",
        "> * lasso polynomial regressor of degree 3 with $\\alpha$ = 0.01\n",
        "> * ridge polynomial regressor of degree 4 with $\\alpha$ = 0.1\n",
        "> * ridge polynomial regressor of degree 3 with $\\alpha$ = 0.1\n",
        "\n",
        "\n",
        "2.   Train on the given training data.\n",
        "3.   Test on the given validation data.\n",
        "4.   Choose the model that performs the best.\n",
        "5.   Test your chosen model on the testing data.\n",
        "***Part 2:***\n",
        "1. Suppose, we don't have the validation data. Use the training data to perform 5-fold validation and then choose the best model.\n",
        "\n"
      ],
      "metadata": {
        "id": "cO7Z33ZGRUV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MSE(y_pred, y): # this is my implementation for mean squared error\n",
        "  return np.sum((y_pred - y)**2)/y_pred.shape[0] "
      ],
      "metadata": {
        "id": "GQLKlR8JXvpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso"
      ],
      "metadata": {
        "id": "NkLszHK7WQOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vandermonde(x, degree): # I have written this function to create the vandermonde matrix X\n",
        "  return np.array([[a**(degree-i) for i in range(degree+1)] for a in x])"
      ],
      "metadata": {
        "id": "fwDmCM3ji8p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def regressor(train_x, train_y, degree, type, alpha = 1.0): # this function returns the right kind of regressor for each model after fitting the data\n",
        "  X = vandermonde(train_x, degree)\n",
        "  reg = None\n",
        "  if type == 1:\n",
        "    reg = LinearRegression().fit(X, train_y)\n",
        "  elif type == 2:\n",
        "    reg = Lasso(alpha).fit(X, train_y)\n",
        "  else:\n",
        "    reg = Ridge(alpha).fit(X, train_y)\n",
        "\n",
        "  return reg"
      ],
      "metadata": {
        "id": "4KLDVb-TilYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def empirical_errors(model, train_x, train_y):\n",
        "  reg = regressor(train_x, train_y, model[\"degree\"], model[\"type\"], model[\"alpha\"]) # for a model, we get the appropriate regressor\n",
        "  X = vandermonde(train_x, model[\"degree\"]) # creating the vandermonde matrix for the training data\n",
        "  train_predict = reg.predict(X) # predicting on the training data\n",
        "  print(model)\n",
        "  print(MSE(train_predict, train_y)) #calculating the mean squared error\n",
        "  return reg"
      ],
      "metadata": {
        "id": "RHd1_brTqUqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [{\"degree\": 4, \"type\": 1, \"alpha\": 0.0}, {\"degree\": 3, \"type\": 1, \"alpha\": 0.0},  \n",
        "          {\"degree\": 4, \"type\": 2, \"alpha\": 0.01}, {\"degree\": 3, \"type\": 2, \"alpha\": 0.01},\n",
        "          {\"degree\": 4, \"type\": 3, \"alpha\": 0.1}, {\"degree\": 3, \"type\": 3, \"alpha\": 0.1}]\n",
        "\n",
        "#here I am expressing each model as a dictionary. A model consists of three parts. It's degree, it's type and it's alpha. I have a list of models containing 6 models\n",
        "\n",
        "regressors = []\n",
        "for i in range(len(models)):\n",
        "  regressors.append(empirical_errors(models[i], train_x, train_y)) # for each of the models, i am looking at the empirical error"
      ],
      "metadata": {
        "id": "1pXwETuqq9rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def errors(reg, model, x, y): # on any dataset, this function predicts output and finds the MSE\n",
        "  X = vandermonde(x, model[\"degree\"]) \n",
        "  predict = reg.predict(X)\n",
        "  print(model)\n",
        "  print(MSE(predict, y))"
      ],
      "metadata": {
        "id": "a9pekB8Jta3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(regressors)): # printing the coefficients for each of the models\n",
        "  print(models[i])\n",
        "  print(regressors[i].coef_)"
      ],
      "metadata": {
        "id": "yQZU1CJCy3d-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(models)):\n",
        "  errors(regressors[i], models[i], validation_x, validation_y) #for each model, we are computing the validation error by calling the errors function.\n"
      ],
      "metadata": {
        "id": "08euHa8vt9hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(models)): # for each model, we are looking at the testing error. we can see that the chosen model in the validation stage performs best here.\n",
        "  errors(regressors[i], models[i], test_x, test_y)"
      ],
      "metadata": {
        "id": "oEura1D60gPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validation(model): # for each model, we are doing 5 fold cross validation\n",
        "  k = 5\n",
        "  fold_size = training_size // k #fold_size is the size of each chunk\n",
        "  error = 0.0\n",
        "  for fold in range(5): #in every iteration\n",
        "    fold_start = fold * fold_size #this is the starting index of the validation chunk\n",
        "    fold_end = min((fold+1)*fold_size - 1, training_size-1) # this is the ending index of the validation chunk\n",
        "    x = y = val_x = val_y = []\n",
        "    for i in range(fold_start): #we copy the dataset upto the starting of the validation chunk to be used for training\n",
        "      x.append(train_x[i])\n",
        "      y.append(train_y[i])\n",
        "    for i in range(fold_end+1, training_size): #we copy the dataset after the end of the validation chunk to be used for training\n",
        "      x.append(train_x[i])\n",
        "      y.append(train_y[i])\n",
        "    for i in range(fold_start, fold_end+1): #we copy the validation chunk to be used for validation\n",
        "      val_x.append(train_x[i])\n",
        "      val_y.append(train_y[i])\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    val_x = np.array(val_x)\n",
        "    val_y = np.array(val_y)\n",
        "    reg = regressor(x, y, model[\"degree\"], model[\"type\"], model[\"alpha\"]) # we pass the training data and the model to get the trained regressor\n",
        "    X = vandermonde(val_x, model[\"degree\"]) #we create the vandermonde matrix on the validation chunk\n",
        "    pred = reg.predict(X) #we predict on the validation chunk\n",
        "    error += MSE(val_y, pred) #we compute the error in this validation chunk and add it to the total\n",
        "  return error/5.0    #we take the average of the 5 validation errors"
      ],
      "metadata": {
        "id": "ARyk6aXAet9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errs = []\n",
        "for model in models:\n",
        "  err = cross_validation(model) # for each of the models, we print the cross validation errors\n",
        "  errs.append(err)\n",
        "\n",
        "print(errs)\n",
        "print('we can see that model ' + str(np.argmin(errs) + 1) + ' performs the best in cross validation')"
      ],
      "metadata": {
        "id": "wpvdMcHmjKA0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}